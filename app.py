import streamlit as st
import cv2
import numpy as np
from ultralytics import YOLO
from PIL import Image
import io
import base64
import time
import os
import warnings
import torch
import random
import tempfile

# ê²½ê³  ì œê±° ë° ë¡œê·¸ ìµœì†Œí™”
warnings.filterwarnings('ignore')
os.environ['YOLO_VERBOSE'] = 'False'

# ì‹œë“œ ê³ ì •
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì”ë°˜ íƒì§€ê¸°",
    page_icon="ğŸ´",
    layout="wide",
    initial_sidebar_state="collapsed",
)

@st.cache_resource(show_spinner=True)
def load_model():
    try:
        with st.spinner("ğŸ¤– AI ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘..."):
            model = YOLO('best.pt')  # YOLOv8 í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
        return model, "âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ"
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None, f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"

def detect_objects_consistent(image_bytes, confidence_threshold=0.5):
    model, status = load_model()
    if model is None:
        return None, [], status

    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
            tmp.write(image_bytes)
            tmp_path = tmp.name

        random.seed(42)
        label_colors = [(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for _ in model.names]

        image = cv2.imread(tmp_path)
        annotated_image = image.copy()

        outputs = model.predict(source=tmp_path, conf=confidence_threshold, iou=0.5)
        results = outputs[0].cpu().numpy()

        detections = []
        for i, det in enumerate(results.boxes.xyxy):
            x1, y1, x2, y2 = map(int, det)
            label_idx = int(results.boxes.cls[i])
            conf = round(float(results.boxes.conf[i]), 2)
            label = model.names[label_idx]
            box_color = label_colors[label_idx]

            label_text = f"{label} {conf:.2f}"
            cv2.rectangle(annotated_image, (x1, y1), (x2, y2), box_color, 20, cv2.LINE_AA)
            (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            cv2.rectangle(annotated_image, (x1, y1 - th - 3), (x1 + tw, y1), box_color, -1)
            cv2.putText(annotated_image, label_text, (x1, y1 - 3), cv2.FONT_HERSHEY_SIMPLEX, 2.2, (0, 0, 0), 8)

            detections.append({
                "class_name": label,
                "confidence": conf,
                "bbox": [x1, y1, x2, y2],
                "area": (x2 - x1) * (y2 - y1),
                "class_id": label_idx
            })

        annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
        return Image.fromarray(annotated_image_rgb), detections, "âœ… ê°ì§€ ì™„ë£Œ"

    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None, [], f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"

def display_detection_stats(detections):
    if not detections:
        return

    class_counts = {}
    total_area = 0
    for det in detections:
        class_name = det['class_name']
        class_counts[class_name] = class_counts.get(class_name, 0) + 1
        total_area += det['area']

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ğŸ“Š ì´ ê°ì²´", len(detections))
    with col2:
        st.metric("ğŸ·ï¸ í´ë˜ìŠ¤ ìˆ˜", len(class_counts))
    with col3:
        avg_conf = sum(det['confidence'] for det in detections) / len(detections)
        st.metric("ğŸ¯ í‰ê·  ì •í™•ë„", f"{avg_conf:.1%}")
    with col4:
        max_conf = max(det['confidence'] for det in detections)
        st.metric("â­ ìµœê³  ì •í™•ë„", f"{max_conf:.1%}")

def process_single_image(uploaded_file, confidence_threshold):
    file_size_mb = uploaded_file.size / (1024 * 1024)
    st.write(f"**ğŸ“Š íŒŒì¼ í¬ê¸°:** {file_size_mb:.2f} MB")
    if file_size_mb > 10:
        st.error("âŒ íŒŒì¼ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
        return

    image_bytes = uploaded_file.read()
    with st.spinner("ğŸ” ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘..."):
        annotated_image, detections, status = detect_objects_consistent(image_bytes, confidence_threshold)

    if annotated_image is not None:
        col1, col2 = st.columns([1, 1])
        with col1:
            st.subheader("ğŸ“¸ ì›ë³¸ ì´ë¯¸ì§€")
            original_image = Image.open(io.BytesIO(image_bytes))
            st.image(original_image, use_container_width=True)
        with col2:
            st.subheader(f"ğŸ¯ ê°ì§€ ê²°ê³¼ ({len(detections)}ê°œ ê°ì²´)")
            st.image(annotated_image, use_container_width=True)

        if detections:
            st.markdown("---")
            st.subheader("ğŸ“Š ê°ì§€ í†µê³„")
            display_detection_stats(detections)

            st.subheader("ğŸ“‹ ê°ì§€ëœ ê°ì²´ ìƒì„¸ ì •ë³´")
            sorted_detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
            for i, detection in enumerate(sorted_detections, 1):
                with st.expander(f"ğŸ·ï¸ {i}. {detection['class_name']} ({detection['confidence']:.1%})"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write(f"**ì •í™•ë„:** {detection['confidence']:.3f}")
                        st.write(f"**í´ë˜ìŠ¤ ID:** {detection['class_id']}")
                    with col2:
                        width = detection['bbox'][2] - detection['bbox'][0]
                        height = detection['bbox'][3] - detection['bbox'][1]
                        st.write(f"**í¬ê¸°:** {width} Ã— {height} px")
                        st.write(f"**ë©´ì :** {detection['area']:,} pixels")
                    st.write(f"**ìœ„ì¹˜:** ({detection['bbox'][0]}, {detection['bbox'][1]}) â†’ ({detection['bbox'][2]}, {detection['bbox'][3]})")
        else:
            st.warning("ğŸ¤” ê°ì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤. ì •í™•ë„ ì„ê³„ê°’ì„ ë‚®ì¶°ë³´ì„¸ìš”.")
    else:
        st.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {status}")

def process_video(video_file, confidence_threshold):
    model, status = load_model()
    if model is None:
        st.error(status)
        return

    tfile = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    tfile.write(video_file.read())
    tfile_path = tfile.name

    cap = cv2.VideoCapture(tfile_path)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    output_path = tfile_path.replace(".mp4", "_output.mp4")
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    progress = st.progress(0)
    frame_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        results = model.predict(source=frame, conf=confidence_threshold, iou=0.5)[0]
        for i, det in enumerate(results.boxes.xyxy):
            x1, y1, x2, y2 = map(int, det)
            label_idx = int(results.boxes.cls[i])
            conf = round(float(results.boxes.conf[i]), 2)
            label = model.names[label_idx]
            color = (0, 255, 0)
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            cv2.putText(frame, f"{label} {conf:.2f}", (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 2, color, 10)

        out.write(frame)
        frame_count += 1
        progress.progress(min(frame_count / total_frames, 1.0))

    cap.release()
    out.release()

    st.success("âœ… ì˜ìƒ ì²˜ë¦¬ ì™„ë£Œ!")
    st.video(output_path)

def main():
    st.title("ğŸ´ ì”ë°˜ íƒì§€ê¸°")
    st.markdown("""
    ### ğŸ¤– YOLOv8 ê¸°ë°˜ ê°ì²´ ê°ì§€ ì‹œìŠ¤í…œ
    ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì˜ìƒì„ ë¶„ì„í•˜ì—¬ ë‹¤ì–‘í•œ ê°ì²´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    """)

    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        confidence_threshold = st.slider("ğŸ¯ ì •í™•ë„ ì„ê³„ê°’", 0.1, 1.0, 0.5, 0.05)

    tab1, tab2 = st.tabs(["ğŸ“· ì´ë¯¸ì§€ ë¶„ì„", "ğŸ¥ ì˜ìƒ ë¶„ì„"])

    with tab1:
        st.header("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ")
        uploaded_files = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”", type=['jpg', 'jpeg', 'png', 'webp'], accept_multiple_files=True)

        if uploaded_files:
            if len(uploaded_files) == 1:
                process_single_image(uploaded_files[0], confidence_threshold)
            else:
                st.success(f"ğŸ“ {len(uploaded_files)}ê°œì˜ ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                tab_titles = [f"ğŸ“¸ ì´ë¯¸ì§€ {i+1}" for i in range(len(uploaded_files))]
                image_tabs = st.tabs(tab_titles)
                for i, (tab, uploaded_file) in enumerate(zip(image_tabs, uploaded_files)):
                    with tab:
                        st.markdown(f"## {tab_titles[i]}")
                        process_single_image(uploaded_file, confidence_threshold)
        else:
            st.info("ğŸ’¡ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì—¬ ê°ì²´ ê°ì§€ë¥¼ ì‹œì‘í•˜ì„¸ìš”!")

    with tab2:
        st.header("ğŸ¥ ì˜ìƒ ì—…ë¡œë“œ")
        uploaded_video = st.file_uploader("ğŸ¬ ë¶„ì„í•  ì˜ìƒ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['mp4', 'avi', 'mov'])

        if uploaded_video is not None:
            st.video(uploaded_video)
            with st.spinner("ğŸ” ì˜ìƒì„ ë¶„ì„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                process_video(uploaded_video, confidence_threshold)
        else:
            st.info("ğŸ’¡ ë¶„ì„í•  ì˜ìƒ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”!")

if __name__ == "__main__":
    main()
